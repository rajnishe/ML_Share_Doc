{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data from review files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewDataPath = {'yelp': 'data/yelp_labelled.txt',\n",
    "                 'amazon': 'data/amazon_cells_labelled.txt',\n",
    "                 'imdb': 'data/imdb_labelled.txt'}\n",
    "reviewList = []\n",
    "\n",
    "for source, filepath in reviewDataPath.items():\n",
    "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
    "    # Add another column filled with the source name\n",
    "    df['source'] = source \n",
    "    reviewList.append(df)\n",
    "\n",
    "df = pd.concat(reviewList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  label source\n",
      "0                           Wow... Loved this place.      1   yelp\n",
      "1                                 Crust is not good.      0   yelp\n",
      "2          Not tasty and the texture was just nasty.      0   yelp\n",
      "3  Stopped by during the late May bank holiday of...      1   yelp\n",
      "4  The selection on the menu was great and so wer...      1   yelp\n",
      "5     Now I am getting angry and I want my damn pho.      0   yelp\n",
      "6              Honeslty it didn't taste THAT fresh.)      0   yelp\n",
      "7  The potatoes were like rubber and you could te...      0   yelp\n",
      "8                          The fries were great too.      1   yelp\n",
      "9                                     A great touch.      1   yelp\n"
     ]
    }
   ],
   "source": [
    "# Just a look at data\n",
    "print(df.iloc[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_yelp = df[df['source'] == 'yelp']\n",
    "\n",
    "sentences = review_yelp['sentence'].values\n",
    "\n",
    "y = review_yelp['label'].values\n",
    "\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "    sentences, y, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_train.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 1 because of reserved 0 index\n",
    "# The indexing is ordered after the most common words in the text, \n",
    "# which you can see by the word the having the index 1. \n",
    "# It is important to note that the index 0 is reserved \n",
    "# and is not assigned to any word. This zero index is used for padding,\n",
    "# because every statement is not of same size\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1747\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sorry, I will not be getting food from here anytime soon :('\n",
      " 'Of all the dishes, the salmon was the best, but all were great.'\n",
      " 'The fries were not hot, and neither was my burger.'\n",
      " \"In fact I'm going to round up to 4 stars, just because she was so awesome.\"\n",
      " 'Will go back next trip out.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences_train[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[740, 4, 46, 12, 20, 160, 10, 72, 35, 355, 232]\n",
      "[11, 43, 1, 171, 1, 283, 3, 1, 47, 26, 43, 24, 22]\n",
      "[1, 233, 24, 12, 209, 2, 741, 3, 23, 125]\n",
      "[14, 356, 83, 126, 5, 742, 59, 5, 357, 96, 41, 127, 234, 3, 25, 161]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1])\n",
    "print(X_train[2])\n",
    "print(X_train[3])\n",
    "print(X_train[4])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "With Tokenizer, the resulting vectors equal the length of each text, and the numbers donâ€™t denote counts,\n",
    "but rather correspond to the word values from the dictionary tokenizer.word_index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAD Sequance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Each text sequence has in most cases different length of words. \n",
    "To counter this, pad_sequence() is used ,which simply pads the sequence of words with zeros. \n",
    "By default, it prepends zeros but we want to append them.\n",
    "Typically it does not matter whether you prepend or append zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxlen parameter to specify how long the sequences should be. \n",
    "#This cuts sequences that exceed that number.\n",
    "\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[740   4  46  12  20 160  10  72  35 355 232   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14 356  83 126   5 742  59   5 357  96  41 127 234   3  25 161   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 100, 50)           87350     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                50010     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 137,371\n",
      "Trainable params: 137,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# vocab size is 1750 \n",
    "# input_length is size of review text after tokenization and pad sequance\n",
    "embedding_dim = 50\n",
    "\n",
    "\n",
    "model.add(layers.Embedding(input_dim=vocab_size,\n",
    "                           output_dim=embedding_dim,\n",
    "                           input_length=maxlen))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 750 samples, validate on 250 samples\n",
      "Epoch 1/20\n",
      "750/750 [==============================] - 0s 390us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.7314 - val_acc: 0.7560\n",
      "Epoch 2/20\n",
      "750/750 [==============================] - 0s 381us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.7398 - val_acc: 0.7560\n",
      "Epoch 3/20\n",
      "750/750 [==============================] - 0s 386us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.7486 - val_acc: 0.7600\n",
      "Epoch 4/20\n",
      "750/750 [==============================] - 0s 384us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.7569 - val_acc: 0.7600\n",
      "Epoch 5/20\n",
      "750/750 [==============================] - 0s 381us/step - loss: 9.6478e-04 - acc: 1.0000 - val_loss: 0.7657 - val_acc: 0.7520\n",
      "Epoch 6/20\n",
      "750/750 [==============================] - 0s 368us/step - loss: 8.5043e-04 - acc: 1.0000 - val_loss: 0.7730 - val_acc: 0.7480\n",
      "Epoch 7/20\n",
      "750/750 [==============================] - 0s 383us/step - loss: 7.6557e-04 - acc: 1.0000 - val_loss: 0.7804 - val_acc: 0.7480\n",
      "Epoch 8/20\n",
      "750/750 [==============================] - 0s 404us/step - loss: 6.9585e-04 - acc: 1.0000 - val_loss: 0.7881 - val_acc: 0.7400\n",
      "Epoch 9/20\n",
      "750/750 [==============================] - 0s 420us/step - loss: 6.3089e-04 - acc: 1.0000 - val_loss: 0.7944 - val_acc: 0.7560\n",
      "Epoch 10/20\n",
      "750/750 [==============================] - 0s 407us/step - loss: 5.8489e-04 - acc: 1.0000 - val_loss: 0.8014 - val_acc: 0.7600\n",
      "Epoch 11/20\n",
      "750/750 [==============================] - 0s 495us/step - loss: 5.3255e-04 - acc: 1.0000 - val_loss: 0.8090 - val_acc: 0.7400\n",
      "Epoch 12/20\n",
      "750/750 [==============================] - 0s 394us/step - loss: 4.8181e-04 - acc: 1.0000 - val_loss: 0.8151 - val_acc: 0.7440\n",
      "Epoch 13/20\n",
      "750/750 [==============================] - 0s 435us/step - loss: 4.4683e-04 - acc: 1.0000 - val_loss: 0.8214 - val_acc: 0.7480\n",
      "Epoch 14/20\n",
      "750/750 [==============================] - 0s 386us/step - loss: 4.1101e-04 - acc: 1.0000 - val_loss: 0.8288 - val_acc: 0.7520\n",
      "Epoch 15/20\n",
      "750/750 [==============================] - 0s 513us/step - loss: 3.7849e-04 - acc: 1.0000 - val_loss: 0.8345 - val_acc: 0.7480\n",
      "Epoch 16/20\n",
      "750/750 [==============================] - 0s 435us/step - loss: 3.5296e-04 - acc: 1.0000 - val_loss: 0.8406 - val_acc: 0.7480\n",
      "Epoch 17/20\n",
      "750/750 [==============================] - 0s 494us/step - loss: 3.2731e-04 - acc: 1.0000 - val_loss: 0.8465 - val_acc: 0.7520\n",
      "Epoch 18/20\n",
      "750/750 [==============================] - 0s 435us/step - loss: 3.0202e-04 - acc: 1.0000 - val_loss: 0.8526 - val_acc: 0.7480\n",
      "Epoch 19/20\n",
      "750/750 [==============================] - 0s 390us/step - loss: 2.8346e-04 - acc: 1.0000 - val_loss: 0.8582 - val_acc: 0.7520\n",
      "Epoch 20/20\n",
      "750/750 [==============================] - 0s 493us/step - loss: 2.6243e-04 - acc: 1.0000 - val_loss: 0.8634 - val_acc: 0.7480\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7480\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy: {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[63, 6, 10, 46, 191, 76]]\n",
      "[[ 63   6  10  46 191  76   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "phrase = \"what a food ,will come again\"\n",
    "\n",
    "tokens = tokenizer.texts_to_sequences([phrase])\n",
    "pad_tokens = pad_sequences(tokens, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(tokens)\n",
    "print(pad_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]], dtype=int32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(pad_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 100, 50)           87350     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 87,871\n",
      "Trainable params: 87,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(layers.Embedding(input_dim=vocab_size,output_dim=embedding_dim,input_length=maxlen))\n",
    "\n",
    "model2.add(layers.GlobalMaxPool1D())\n",
    "\n",
    "model2.add(layers.Dense(10, activation='relu'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "model2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 750 samples, validate on 250 samples\n",
      "Epoch 1/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6903 - acc: 0.5413 - val_loss: 0.6833 - val_acc: 0.6720\n",
      "Epoch 2/20\n",
      "750/750 [==============================] - 0s 623us/step - loss: 0.6556 - acc: 0.8533 - val_loss: 0.6487 - val_acc: 0.7240\n",
      "Epoch 3/20\n",
      "750/750 [==============================] - 0s 563us/step - loss: 0.5646 - acc: 0.9067 - val_loss: 0.5781 - val_acc: 0.7560\n",
      "Epoch 4/20\n",
      "750/750 [==============================] - 0s 560us/step - loss: 0.4238 - acc: 0.9373 - val_loss: 0.5027 - val_acc: 0.7640\n",
      "Epoch 5/20\n",
      "750/750 [==============================] - 0s 557us/step - loss: 0.2874 - acc: 0.9547 - val_loss: 0.4544 - val_acc: 0.7760\n",
      "Epoch 6/20\n",
      "750/750 [==============================] - 0s 556us/step - loss: 0.1890 - acc: 0.9720 - val_loss: 0.4262 - val_acc: 0.8200\n",
      "Epoch 7/20\n",
      "750/750 [==============================] - 0s 665us/step - loss: 0.1226 - acc: 0.9827 - val_loss: 0.4128 - val_acc: 0.8080\n",
      "Epoch 8/20\n",
      "750/750 [==============================] - 0s 648us/step - loss: 0.0796 - acc: 0.9933 - val_loss: 0.4102 - val_acc: 0.8160\n",
      "Epoch 9/20\n",
      "750/750 [==============================] - 0s 542us/step - loss: 0.0519 - acc: 0.9960 - val_loss: 0.4133 - val_acc: 0.8200\n",
      "Epoch 10/20\n",
      "750/750 [==============================] - 0s 529us/step - loss: 0.0355 - acc: 1.0000 - val_loss: 0.4172 - val_acc: 0.8120\n",
      "Epoch 11/20\n",
      "750/750 [==============================] - 0s 519us/step - loss: 0.0251 - acc: 1.0000 - val_loss: 0.4234 - val_acc: 0.8120\n",
      "Epoch 12/20\n",
      "750/750 [==============================] - 0s 535us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.4297 - val_acc: 0.8040\n",
      "Epoch 13/20\n",
      "750/750 [==============================] - 0s 599us/step - loss: 0.0139 - acc: 1.0000 - val_loss: 0.4361 - val_acc: 0.8040\n",
      "Epoch 14/20\n",
      "750/750 [==============================] - 0s 638us/step - loss: 0.0108 - acc: 1.0000 - val_loss: 0.4428 - val_acc: 0.8080\n",
      "Epoch 15/20\n",
      "750/750 [==============================] - 0s 568us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.4499 - val_acc: 0.8120\n",
      "Epoch 16/20\n",
      "750/750 [==============================] - 0s 536us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.4553 - val_acc: 0.8080\n",
      "Epoch 17/20\n",
      "750/750 [==============================] - 0s 543us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.4619 - val_acc: 0.8080\n",
      "Epoch 18/20\n",
      "750/750 [==============================] - 0s 538us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.4684 - val_acc: 0.8120\n",
      "Epoch 19/20\n",
      "750/750 [==============================] - 0s 523us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.4731 - val_acc: 0.8200\n",
      "Epoch 20/20\n",
      "750/750 [==============================] - 0s 527us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.4795 - val_acc: 0.8160\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(X_train, y_train,\n",
    "                    epochs=20,verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model2.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.8160\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model2.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy: {:.4f}\".format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pre-Trained GloVe vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    \n",
    "    vocab_size = len(word_index) + 1 \n",
    "    # Adding again 1 because of reserved 0 index\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as file:\n",
    "        for line in file:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                print(\"{} {} \".format(word,idx))\n",
    "                embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 1 \n",
      "of 11 \n",
      "to 5 \n",
      "and 2 \n",
      "in 14 \n",
      "a 6 \n",
      "for 13 \n",
      "that 27 \n",
      "on 34 \n",
      "is 7 \n",
      "was 3 \n",
      "said 176 \n",
      "with 21 \n",
      "he 68 \n",
      "as 42 \n",
      "it 8 \n",
      "by 71 \n",
      "at 37 \n",
      "from 72 \n",
      "his 268 \n",
      "an 44 \n",
      "be 20 \n",
      "has 94 \n",
      "are 31 \n",
      "have 30 \n",
      "but 26 \n",
      "were 24 \n",
      "not 12 \n",
      "this 9 \n",
      "who 286 \n",
      "they 33 \n",
      "had 28 \n",
      "i 4 \n",
      "which 70 \n",
      "will 46 \n",
      "their 56 \n",
      "or 81 \n",
      "its 1448 \n",
      "one 50 \n",
      "after 138 \n",
      "new 313 \n",
      "been 64 \n",
      "also 53 \n",
      "we 17 \n",
      "would 66 \n",
      "two 240 \n",
      "more 123 \n",
      "first 144 \n",
      "about 93 \n",
      "up 59 \n",
      "when 78 \n",
      "year 694 \n",
      "there 48 \n",
      "all 43 \n",
      "out 58 \n",
      "she 234 \n",
      "other 107 \n",
      "people 170 \n",
      "her 263 \n",
      "than 106 \n",
      "over 121 \n",
      "into 837 \n",
      "last 305 \n",
      "some 101 \n",
      "time 40 \n",
      "you 29 \n",
      "if 51 \n",
      "no 62 \n",
      "can 163 \n",
      "three 650 \n",
      "do 204 \n",
      "only 52 \n",
      "could 112 \n",
      "us 67 \n",
      "so 25 \n",
      "them 148 \n",
      "what 63 \n",
      "him 1566 \n",
      "during 301 \n",
      "before 157 \n",
      "may 401 \n",
      "since 190 \n",
      "many 213 \n",
      "while 217 \n",
      "where 316 \n",
      "because 127 \n",
      "now 136 \n",
      "made 100 \n",
      "like 36 \n",
      "between 1131 \n",
      "did 69 \n",
      "just 41 \n",
      "day 173 \n",
      "under 441 \n",
      "such 714 \n",
      "second 667 \n",
      "then 159 \n",
      "company 881 \n",
      "group 592 \n",
      "any 134 \n",
      "four 1113 \n",
      "being 114 \n",
      "down 153 \n",
      "back 32 \n",
      "off 218 \n",
      "well 89 \n",
      "week 1030 \n",
      "still 167 \n",
      "both 236 \n",
      "even 88 \n",
      "high 417 \n",
      "part 676 \n",
      "told 434 \n",
      "those 1463 \n",
      "end 697 \n",
      "these 306 \n",
      "make 226 \n",
      "work 1693 \n",
      "our 38 \n",
      "home 455 \n",
      "party 432 \n",
      "house 340 \n",
      "old 252 \n",
      "later 721 \n",
      "get 98 \n",
      "another 140 \n",
      "long 341 \n",
      "five 1357 \n",
      "1 183 \n",
      "way 145 \n",
      "used 403 \n",
      "much 85 \n",
      "next 128 \n",
      "here 35 \n",
      "should 280 \n",
      "take 192 \n",
      "very 18 \n",
      "my 23 \n",
      "how 137 \n",
      "public 1432 \n",
      "several 538 \n",
      "court 862 \n",
      "say 135 \n",
      "around 202 \n",
      "10 309 \n",
      "until 502 \n",
      "set 1256 \n",
      "says 1657 \n",
      "market 1287 \n",
      "however 523 \n",
      "family 246 \n",
      "life 1364 \n",
      "left 262 \n",
      "good 16 \n",
      "top 1675 \n",
      "going 126 \n",
      "known 1152 \n",
      "six 1335 \n",
      "dollars 1099 \n",
      "2 188 \n",
      "use 649 \n",
      "each 362 \n",
      "area 247 \n",
      "found 266 \n",
      "sunday 1502 \n",
      "place 15 \n",
      "go 39 \n",
      "third 682 \n",
      "times 130 \n",
      "took 201 \n",
      "right 169 \n",
      "see 214 \n",
      "best 47 \n",
      "business 461 \n",
      "does 1291 \n",
      "came 84 \n",
      "months 1556 \n",
      "power 1582 \n",
      "think 120 \n",
      "service 19 \n",
      "help 1197 \n",
      "me 75 \n",
      "expected 1575 \n",
      "final 1117 \n",
      "added 729 \n",
      "without 1596 \n",
      "white 421 \n",
      "want 102 \n",
      "few 261 \n",
      "money 270 \n",
      "name 1186 \n",
      "3 258 \n",
      "oil 943 \n",
      "too 151 \n",
      "20 336 \n",
      "come 191 \n",
      "chinese 664 \n",
      "town 199 \n",
      "never 55 \n",
      "little 147 \n",
      "prime 913 \n",
      "least 378 \n",
      "past 882 \n",
      "half 452 \n",
      "saying 1271 \n",
      "know 216 \n",
      "great 22 \n",
      "small 429 \n",
      "every 150 \n",
      "head 1460 \n",
      "ago 1322 \n",
      "night 219 \n",
      "big 738 \n",
      "far 365 \n",
      "today 443 \n",
      "although 703 \n",
      "again 76 \n",
      "close 323 \n",
      "went 133 \n",
      "point 494 \n",
      "must 208 \n",
      "your 54 \n",
      "recent 847 \n",
      "whether 838 \n",
      "lost 594 \n",
      "music 708 \n",
      "15 1483 \n",
      "got 103 \n",
      "30 346 \n",
      "need 464 \n",
      "4 357 \n",
      "though 398 \n",
      "might 1649 \n",
      "hit 275 \n",
      "11 1665 \n",
      "away 566 \n",
      "12 705 \n",
      "5 203 \n",
      "others 509 \n",
      "large 293 \n",
      "water 530 \n",
      "deal 335 \n",
      "attack 478 \n",
      "side 228 \n",
      "better 105 \n",
      "once 253 \n",
      "due 1035 \n",
      "building 1384 \n",
      "club 1020 \n",
      "given 1651 \n",
      "give 179 \n",
      "order 113 \n",
      "prices 146 \n",
      "round 742 \n",
      "human 1443 \n",
      "asked 347 \n",
      "total 639 \n",
      "bill 730 \n",
      "further 1283 \n",
      "street 1126 \n",
      "different 590 \n",
      "received 976 \n",
      "return 241 \n",
      "college 1542 \n",
      "working 1740 \n",
      "groups 1625 \n",
      "despite 1445 \n",
      "level 1490 \n",
      "station 522 \n",
      "having 767 \n",
      "services 583 \n",
      "6 1569 \n",
      "visit 326 \n",
      "lot 265 \n",
      "across 865 \n",
      "workers 1523 \n",
      "book 917 \n",
      "fell 608 \n",
      "seen 483 \n",
      "full 343 \n",
      "started 475 \n",
      "yet 498 \n",
      "possible 314 \n",
      "special 260 \n",
      "100 684 \n",
      "behind 473 \n",
      "food 10 \n",
      "real 344 \n",
      "car 1482 \n",
      "rate 1446 \n",
      "enough 139 \n",
      "outside 276 \n",
      "really 45 \n",
      "almost 642 \n",
      "single 701 \n",
      "trying 487 \n",
      "find 556 \n",
      "minutes 82 \n",
      "hard 220 \n",
      "hours 557 \n",
      "cut 591 \n",
      "san 818 \n",
      "price 274 \n",
      "son 1633 \n",
      "key 1498 \n",
      "red 614 \n",
      "average 382 \n",
      "pay 353 \n",
      "something 621 \n",
      "gave 294 \n",
      "2007 1276 \n",
      "8 731 \n",
      "low 712 \n",
      "things 338 \n",
      "ever 65 \n",
      "look 385 \n",
      "job 431 \n",
      "front 1185 \n",
      "list 658 \n",
      "live 391 \n",
      "always 99 \n",
      "playing 662 \n",
      "middle 936 \n",
      "meet 1320 \n",
      "wife 553 \n",
      "ahead 1679 \n",
      "english 1009 \n",
      "closed 878 \n",
      "soon 232 \n",
      "served 297 \n",
      "believe 547 \n",
      "why 369 \n",
      "summer 1264 \n",
      "wanted 470 \n",
      "recently 570 \n",
      "course 558 \n",
      "problem 1663 \n",
      "done 281 \n",
      "star 447 \n",
      "care 1636 \n",
      "gold 1548 \n",
      "dead 1004 \n",
      "opened 1535 \n",
      "instead 1232 \n",
      "daily 1014 \n",
      "coming 142 \n",
      "indian 796 \n",
      "similar 1695 \n",
      "running 322 \n",
      "40 329 \n",
      "thought 257 \n",
      "ground 1407 \n",
      "staff 80 \n",
      "hand 1006 \n",
      "hope 681 \n",
      "returned 1557 \n",
      "love 86 \n",
      "stop 602 \n",
      "try 200 \n",
      "ended 655 \n",
      "management 325 \n",
      "brought 1741 \n",
      "decided 764 \n",
      "gas 1465 \n",
      "fact 356 \n",
      "especially 367 \n",
      "common 1154 \n",
      "manager 471 \n",
      "rather 598 \n",
      "thing 225 \n",
      "getting 160 \n",
      "biggest 1512 \n",
      "let 1506 \n",
      "means 696 \n",
      "turn 926 \n",
      "leave 376 \n",
      "light 1263 \n",
      "person 631 \n",
      "either 319 \n",
      "class 1544 \n",
      "needed 1370 \n",
      "am 311 \n",
      "doing 758 \n",
      "kind 377 \n",
      "airport 964 \n",
      "judge 517 \n",
      "room 394 \n",
      "parties 1295 \n",
      "nothing 221 \n",
      "bring 231 \n",
      "operation 924 \n",
      "events 1607 \n",
      "beat 438 \n",
      "helped 990 \n",
      "probably 143 \n",
      "tried 333 \n",
      "bad 90 \n",
      "green 392 \n",
      "forward 896 \n",
      "poor 237 \n",
      "feel 149 \n",
      "attention 1343 \n",
      "themselves 1377 \n",
      "itself 304 \n",
      "wall 633 \n",
      "23 1032 \n",
      "wants 1471 \n",
      "italian 609 \n",
      "rest 911 \n",
      "inside 248 \n",
      "immediately 469 \n",
      "sides 375 \n",
      "heart 287 \n",
      "version 845 \n",
      "step 895 \n",
      "above 883 \n",
      "needs 397 \n",
      "talk 238 \n",
      "joint 875 \n",
      "sure 235 \n",
      "stay 1166 \n",
      "worth 193 \n",
      "charge 1664 \n",
      "begin 1065 \n",
      "friends 554 \n",
      "anything 596 \n",
      "sign 1466 \n",
      "claimed 993 \n",
      "whole 577 \n",
      "cannot 605 \n",
      "break 688 \n",
      "daughter 1349 \n",
      "strike 1470 \n",
      "style 704 \n",
      "paid 1269 \n",
      "rock 893 \n",
      "arrived 410 \n",
      "hour 272 \n",
      "value 657 \n",
      "professional 503 \n",
      "everything 194 \n",
      "weekend 1692 \n",
      "quickly 718 \n",
      "blue 1114 \n",
      "drive 695 \n",
      "experience 87 \n",
      "giving 1058 \n",
      "finally 702 \n",
      "magazine 1425 \n",
      "employees 1025 \n",
      "mostly 1721 \n",
      "reason 1338 \n",
      "overall 259 \n",
      "passed 635 \n",
      "bay 371 \n",
      "wide 1191 \n",
      "below 1404 \n",
      "double 520 \n",
      "hands 368 \n",
      "review 476 \n",
      "kept 337 \n",
      "ordered 119 \n",
      "limited 1480 \n",
      "trip 358 \n",
      "paper 1205 \n",
      "concern 931 \n",
      "someone 560 \n",
      "huge 732 \n",
      "husband 384 \n",
      "cover 1676 \n",
      "couple 727 \n",
      "complete 952 \n",
      "non 1499 \n",
      "seems 1385 \n",
      "felt 302 \n",
      "hopes 1028 \n",
      "tell 350 \n",
      "winner 1734 \n",
      "serve 958 \n",
      "sense 1155 \n",
      "regular 1577 \n",
      "word 1373 \n",
      "everyone 407 \n",
      "actually 585 \n",
      "friend 722 \n",
      "amount 381 \n",
      "ways 1433 \n",
      "places 243 \n",
      "join 1019 \n",
      "heard 680 \n",
      "entire 1514 \n",
      "valley 1423 \n",
      "read 1456 \n",
      "perhaps 826 \n",
      "deep 942 \n",
      "simply 617 \n",
      "relationship 1294 \n",
      "cash 1217 \n",
      "alone 1507 \n",
      "lack 1372 \n",
      "twice 400 \n",
      "songs 1202 \n",
      "scene 1002 \n",
      "afternoon 1571 \n",
      "quality 155 \n",
      "sound 1389 \n",
      "spring 1146 \n",
      "missing 986 \n",
      "beyond 783 \n",
      "francisco 819 \n",
      "section 915 \n",
      "gone 599 \n",
      "s 987 \n",
      "opportunity 880 \n",
      "expect 352 \n",
      "send 765 \n",
      "host 1371 \n",
      "avoid 300 \n",
      "consider 832 \n",
      "stars 96 \n",
      "lived 834 \n",
      "happened 632 \n",
      "largely 1379 \n",
      "businesses 1447 \n",
      "bit 196 \n",
      "fast 414 \n",
      "boy 595 \n",
      "worst 156 \n",
      "70 992 \n",
      "greek 444 \n",
      "finish 462 \n",
      "hot 209 \n",
      "seat 769 \n",
      "writing 1262 \n",
      "fear 841 \n",
      "customers 724 \n",
      "sun 1599 \n",
      "particular 848 \n",
      "mind 1087 \n",
      "35 1036 \n",
      "quite 215 \n",
      "fine 430 \n",
      "serving 628 \n",
      "ten 1738 \n",
      "seemed 568 \n",
      "bus 1005 \n",
      "ask 449 \n",
      "degree 825 \n",
      "watch 480 \n",
      "lawyers 861 \n",
      "ice 298 \n",
      "listed 1662 \n",
      "else 386 \n",
      "rich 1722 \n",
      "offers 604 \n",
      "evening 1148 \n",
      "happy 320 \n",
      "giant 885 \n",
      "larger 1568 \n",
      "article 1455 \n",
      "managed 794 \n",
      "bought 1534 \n",
      "smaller 1095 \n",
      "owner 606 \n",
      "highly 492 \n",
      "performed 752 \n",
      "wrong 330 \n",
      "table 181 \n",
      "looked 477 \n",
      "towards 571 \n",
      "note 1211 \n",
      "kids 1081 \n",
      "rice 289 \n",
      "super 308 \n",
      "stopped 854 \n",
      "picture 1704 \n",
      "spot 205 \n",
      "guy 472 \n",
      "cold 224 \n",
      "signs 1215 \n",
      "struck 1718 \n",
      "maybe 387 \n",
      "90 1406 \n",
      "door 562 \n",
      "apparently 1359 \n",
      "talking 1376 \n",
      "fresh 131 \n",
      "caught 827 \n",
      "pretty 104 \n",
      "setting 613 \n",
      "box 1469 \n",
      "orders 1516 \n",
      "provides 874 \n",
      "via 1021 \n",
      "accident 1171 \n",
      "mary 1595 \n",
      "block 1350 \n",
      "heat 536 \n",
      "heads 1027 \n",
      "bowl 491 \n",
      "ones 1001 \n",
      "extra 619 \n",
      "voted 1422 \n",
      "covered 1609 \n",
      "spend 574 \n",
      "hits 1089 \n",
      "location 291 \n",
      "slow 158 \n",
      "bar 177 \n",
      "stood 1064 \n",
      "mexican 576 \n",
      "waiting 230 \n",
      "fish 687 \n",
      "boys 1151 \n",
      "none 500 \n",
      "missed 1281 \n",
      "unless 559 \n",
      "strip 415 \n",
      "simple 1238 \n",
      "foot 836 \n",
      "t 899 \n",
      "opinion 1573 \n",
      "christmas 1509 \n",
      "buying 928 \n",
      "mixed 1545 \n",
      "leaves 969 \n",
      "philadelphia 1629 \n",
      "miss 1627 \n",
      "completely 737 \n",
      "putting 1210 \n",
      "owners 532 \n",
      "baby 646 \n",
      "dark 1602 \n",
      "neither 741 \n",
      "pulled 573 \n",
      "returning 1525 \n",
      "plus 1085 \n",
      "grow 1096 \n",
      "thai 303 \n",
      "guys 1631 \n",
      "weekly 849 \n",
      "shots 759 \n",
      "falling 1702 \n",
      "focused 1457 \n",
      "rare 317 \n",
      "die 465 \n",
      "fly 665 \n",
      "piece 411 \n",
      "check 197 \n",
      "honor 1272 \n",
      "bottom 1013 \n",
      "officially 1685 \n",
      "classic 1265 \n",
      "disaster 1332 \n",
      "neighborhood 709 \n",
      "pace 641 \n",
      "golden 1296 \n",
      "count 1468 \n",
      "clean 185 \n",
      "expectations 1321 \n",
      "handed 1660 \n",
      "movies 1292 \n",
      "wait 110 \n",
      "color 666 \n",
      "pictures 1565 \n",
      "seriously 416 \n",
      "feeling 1597 \n",
      "downtown 528 \n",
      "considering 424 \n",
      "acknowledged 612 \n",
      "restaurant 49 \n",
      "website 925 \n",
      "pop 496 \n",
      "doubt 927 \n",
      "friendly 60 \n",
      "venture 1282 \n",
      "quick 858 \n",
      "mile 1418 \n",
      "1979 835 \n",
      "greatest 1382 \n",
      "perfect 166 \n",
      "asking 1063 \n",
      "bird 1412 \n",
      "garden 1040 \n",
      "elsewhere 575 \n",
      "struggle 863 \n",
      "surprise 1522 \n",
      "corporation 1440 \n",
      "wave 864 \n",
      "weak 991 \n",
      "favorite 419 \n",
      "extremely 278 \n",
      "memory 1077 \n",
      "treated 408 \n",
      "remember 1511 \n",
      "apart 1703 \n",
      "southwest 1069 \n",
      "otherwise 1438 \n",
      "wine 406 \n",
      "worse 440 \n",
      "options 379 \n",
      "lady 526 \n",
      "multiple 467 \n",
      "changing 1552 \n",
      "del 1312 \n",
      "serves 484 \n",
      "yellow 971 \n",
      "glass 1394 \n",
      "expanded 1301 \n",
      "shop 1141 \n",
      "fun 645 \n",
      "expensive 1690 \n",
      "hole 1125 \n",
      "dog 706 \n",
      "portion 506 \n",
      "salt 700 \n",
      "flat 418 \n",
      "concept 1653 \n",
      "sitting 459 \n",
      "unique 1208 \n",
      "choose 580 \n",
      "walked 507 \n",
      "nice 57 \n",
      "bigger 1667 \n",
      "thanks 897 \n",
      "watched 395 \n",
      "establishment 413 \n",
      "stayed 1368 \n",
      "medium 1159 \n",
      "sugar 891 \n",
      "oh 626 \n",
      "palm 1164 \n",
      "worker 1184 \n",
      "inspired 829 \n",
      "sat 256 \n",
      "rating 1260 \n",
      "dedicated 1307 \n",
      "touch 1581 \n",
      "myself 1242 \n",
      "covers 770 \n",
      "maria 744 \n",
      "preparing 481 \n",
      "gross 1319 \n",
      "dry 290 \n",
      "apple 1225 \n",
      "prepare 939 \n",
      "roll 1268 \n",
      "gordon 809 \n",
      "healthy 422 \n",
      "experienced 996 \n",
      "blow 1118 \n",
      "promise 555 \n",
      "dinner 277 \n",
      "stepped 514 \n",
      "beautiful 539 \n",
      "sharply 812 \n",
      "extensive 873 \n",
      "combination 686 \n",
      "angry 1635 \n",
      "walls 1608 \n",
      "obviously 1555 \n",
      "suggest 1254 \n",
      "coffee 1143 \n",
      "deliver 922 \n",
      "dealing 1134 \n",
      "cool 1150 \n",
      "wish 1628 \n",
      "register 1497 \n",
      "familiar 1630 \n",
      "regularly 947 \n",
      "stuff 1045 \n",
      "circumstances 1188 \n",
      "usual 1194 \n",
      "outstanding 442 \n",
      "contain 1532 \n",
      "warm 405 \n",
      "def 870 \n",
      "surprised 1454 \n",
      "actual 1390 \n",
      "deeply 963 \n",
      "besides 1380 \n",
      "phoenix 295 \n",
      "eat 91 \n",
      "waste 345 \n",
      "complaints 1428 \n",
      "pan 1328 \n",
      "typical 1355 \n",
      "employee 1369 \n",
      "drawing 1367 \n",
      "reviews 504 \n",
      "selection 332 \n",
      "enjoy 264 \n",
      "sweet 351 \n",
      "handling 1156 \n",
      "restaurants 593 \n",
      "fat 1746 \n",
      "cape 1585 \n",
      "somewhat 909 \n",
      "constructed 1477 \n",
      "couples 1624 \n",
      "meat 245 \n",
      "fill 1339 \n",
      "please 1261 \n",
      "rapidly 1097 \n",
      "chip 1467 \n",
      "mention 1731 \n",
      "feels 723 \n",
      "packed 1539 \n",
      "recommended 493 \n",
      "qualified 844 \n",
      "mistake 1616 \n",
      "atmosphere 184 \n",
      "empty 360 \n",
      "definitely 92 \n",
      "smoke 1614 \n",
      "shops 817 \n",
      "correct 1206 \n",
      "absolutely 178 \n",
      "enjoyed 269 \n",
      "brings 1075 \n",
      "interesting 579 \n",
      "zero 383 \n",
      "rarely 678 \n",
      "guests 956 \n",
      "comfortable 906 \n",
      "burned 1707 \n",
      "requested 1203 \n",
      "rated 683 \n",
      "fruit 1401 \n",
      "drinking 1341 \n",
      "loved 174 \n",
      "stretch 1128 \n",
      "lunch 162 \n",
      "mouth 433 \n",
      "milk 1248 \n",
      "beer 334 \n",
      "customer 321 \n",
      "eve 1510 \n",
      "lots 535 \n",
      "barely 282 \n",
      "disappointed 108 \n",
      "busy 490 \n",
      "chicken 115 \n",
      "treat 1110 \n",
      "tea 296 \n",
      "vegas 74 \n",
      "beef 349 \n",
      "excellent 210 \n",
      "fail 1513 \n",
      "google 1100 \n",
      "plate 651 \n",
      "totally 510 \n",
      "taste 154 \n",
      "legs 720 \n",
      "attached 1464 \n",
      "tonight 1473 \n",
      "par 1526 \n",
      "bars 1311 \n",
      "summary 1378 \n",
      "thin 439 \n",
      "sick 519 \n",
      "conclusion 1719 \n",
      "indicate 1216 \n",
      "eating 168 \n",
      "fairly 654 \n",
      "arriving 1133 \n",
      "impressive 876 \n",
      "realized 567 \n",
      "drink 463 \n",
      "bloody 1594 \n",
      "tragedy 1717 \n",
      "personally 1710 \n",
      "wings 457 \n",
      "staying 603 \n",
      "truly 871 \n",
      "hip 1689 \n",
      "extraordinary 1051 \n",
      "unfortunately 331 \n",
      "corn 1549 \n",
      "trips 1578 \n",
      "spots 1309 \n",
      "readers 1424 \n",
      "imagine 1101 \n",
      "pleased 1528 \n",
      "sporting 1606 \n",
      "saving 1726 \n",
      "piano 1488 \n",
      "chips 239 \n",
      "rushed 1472 \n",
      "ok 698 \n",
      "mediterranean 1149 \n",
      "update 1405 \n",
      "thick 1584 \n",
      "dressed 1640 \n",
      "venue 1623 \n",
      "foods 846 \n",
      "cooking 1543 \n",
      "guess 647 \n",
      "maine 1266 \n",
      "ourselves 1700 \n",
      "gotten 975 \n",
      "metro 789 \n",
      "equally 980 \n",
      "shirt 1115 \n",
      "tip 525 \n",
      "ignored 1240 \n",
      "eggs 638 \n",
      "pack 746 \n",
      "cream 299 \n",
      "frozen 584 \n",
      "strange 930 \n",
      "basically 949 \n",
      "wonderful 222 \n",
      "mac 1298 \n",
      "smooth 804 \n",
      "bone 1682 \n",
      "reasonable 456 \n",
      "sad 450 \n",
      "shall 811 \n",
      "warnings 1530 \n",
      "handled 994 \n",
      "everywhere 1410 \n",
      "ratio 779 \n",
      "casino 894 \n",
      "plain 772 \n",
      "disappointing 315 \n",
      "anyway 541 \n",
      "forever 1600 \n",
      "multi 1219 \n",
      "occasions 1158 \n",
      "satisfied 1598 \n",
      "funny 1561 \n",
      "pink 1112 \n",
      "terrible 206 \n",
      "sub 1668 \n",
      "crowds 859 \n",
      "rubber 1678 \n",
      "liked 420 \n",
      "crazy 527 \n",
      "grain 1220 \n",
      "shocked 1214 \n",
      "arrives 934 \n",
      "prefer 1352 \n",
      "ranch 1178 \n",
      "amazing 61 \n",
      "lined 1249 \n",
      "cheese 426 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diverse 1057 \n",
      "subway 451 \n",
      "touched 1147 \n",
      "wound 1366 \n",
      "bread 615 \n",
      "madison 855 \n",
      "pepper 1228 \n",
      "proven 1003 \n",
      "literally 423 \n",
      "indoor 1039 \n",
      "hearts 1163 \n",
      "joy 1076 \n",
      "99 1666 \n",
      "breakfast 165 \n",
      "forth 989 \n",
      "waited 207 \n",
      "impressed 227 \n",
      "brick 672 \n",
      "mom 816 \n",
      "honest 669 \n",
      "receives 750 \n",
      "absolute 1198 \n",
      "somehow 1326 \n",
      "fails 921 \n",
      "folks 692 \n",
      "chocolate 1247 \n",
      "dirty 292 \n",
      "avoided 1313 \n",
      "presentation 540 \n",
      "whenever 1017 \n",
      "witnessed 955 \n",
      "sorry 740 \n",
      "overhaul 953 \n",
      "thirty 1638 \n",
      "butter 1223 \n",
      "cow 533 \n",
      "finger 801 \n",
      "soundtrack 1489 \n",
      "flower 640 \n",
      "theft 833 \n",
      "letting 1116 \n",
      "egg 366 \n",
      "duo 1200 \n",
      "disappointment 324 \n",
      "array 1192 \n",
      "loves 1681 \n",
      "suggestions 745 \n",
      "fare 587 \n",
      "item 802 \n",
      "han 776 \n",
      "rings 1290 \n",
      "poorly 1476 \n",
      "teeth 787 \n",
      "meal 172 \n",
      "edinburgh 999 \n",
      "blown 1604 \n",
      "sauce 152 \n",
      "vegetables 354 \n",
      "highlighted 1207 \n",
      "cafe 1521 \n",
      "checked 1255 \n",
      "priced 374 \n",
      "juice 1226 \n",
      "leather 1337 \n",
      "perfectly 396 \n",
      "greens 1162 \n",
      "ingredients 1318 \n",
      "greeted 565 \n",
      "recommendation 542 \n",
      "ignore 1344 \n",
      "filling 734 \n",
      "dining 229 \n",
      "tables 328 \n",
      "dish 187 \n",
      "luke 982 \n",
      "chef 437 \n",
      "ladies 1661 \n",
      "meals 390 \n",
      "bunch 578 \n",
      "dirt 551 \n",
      "drinks 412 \n",
      "joke 1645 \n",
      "tender 327 \n",
      "glad 534 \n",
      "grandmother 1243 \n",
      "salmon 283 \n",
      "complain 460 \n",
      "incredible 307 \n",
      "stomach 359 \n",
      "decorated 1642 \n",
      "rolls 372 \n",
      "bargain 1047 \n",
      "ordering 1132 \n",
      "lacked 399 \n",
      "monster 1709 \n",
      "recommend 198 \n",
      "mess 904 \n",
      "appealing 1672 \n",
      "pork 389 \n",
      "pale 1231 \n",
      "stir 1196 \n",
      "furthermore 923 \n",
      "hungry 1175 \n",
      "generous 505 \n",
      "duck 1111 \n",
      "portions 586 \n",
      "fantastic 141 \n",
      "cake 1043 \n",
      "cartel 1142 \n",
      "tolerance 1435 \n",
      "server 124 \n",
      "poured 1365 \n",
      "delicate 1564 \n",
      "spends 1375 \n",
      "menu 97 \n",
      "lacking 622 \n",
      "excuse 643 \n",
      "forty 1356 \n",
      "relaxed 1622 \n",
      "soup 404 \n",
      "boyfriend 644 \n",
      "flavor 118 \n",
      "decent 610 \n",
      "lightly 888 \n",
      "reminded 1067 \n",
      "sergeant 1227 \n",
      "relocated 1524 \n",
      "helpful 342 \n",
      "ha 1558 \n",
      "ripped 699 \n",
      "affordable 1501 \n",
      "pricing 563 \n",
      "lover 1487 \n",
      "seasonal 1400 \n",
      "beans 361 \n",
      "dressing 445 \n",
      "dishes 171 \n",
      "dried 754 \n",
      "shoe 1336 \n",
      "profound 1612 \n",
      "bare 940 \n",
      "yeah 1706 \n",
      "con 1686 \n",
      "trap 831 \n",
      "assure 1023 \n",
      "tops 1189 \n",
      "pleasant 427 \n",
      "vacant 1639 \n",
      "attitudes 625 \n",
      "loving 1632 \n",
      "drunk 1137 \n",
      "desired 970 \n",
      "deserves 453 \n",
      "stupid 775 \n",
      "garlic 735 \n",
      "correction 1302 \n",
      "frenchman 997 \n",
      "wash 1437 \n",
      "lovers 668 \n",
      "sticks 677 \n",
      "generic 1168 \n",
      "oven 673 \n",
      "cooked 223 \n",
      "enthusiastic 1109 \n",
      "overwhelmed 588 \n",
      "tongue 868 \n",
      "imagination 1129 \n",
      "lighter 1431 \n",
      "lemon 851 \n",
      "bite 799 \n",
      "classics 901 \n",
      "prompt 757 \n",
      "pizza 95 \n",
      "dos 1169 \n",
      "potatoes 675 \n",
      "bathroom 561 \n",
      "suffers 1727 \n",
      "strings 1012 \n",
      "salad 122 \n",
      "pile 1411 \n",
      "cart 1520 \n",
      "experiencing 1293 \n",
      "tracked 1409 \n",
      "instantly 1060 \n",
      "dylan 898 \n",
      "poisoning 768 \n",
      "editing 763 \n",
      "verge 766 \n",
      "promptly 1420 \n",
      "seated 267 \n",
      "awful 1724 \n",
      "boot 791 \n",
      "quantity 1090 \n",
      "bucks 1459 \n",
      "bacon 486 \n",
      "horrible 279 \n",
      "chickens 1026 \n",
      "sore 788 \n",
      "weird 1562 \n",
      "grill 607 \n",
      "apologize 1007 \n",
      "eaten 285 \n",
      "judging 828 \n",
      "min 636 \n",
      "potato 448 \n",
      "ridiculous 1395 \n",
      "fried 288 \n",
      "ate 543 \n",
      "stuffed 1176 \n",
      "accordingly 1723 \n",
      "relax 1304 \n",
      "banana 1353 \n",
      "nasty 691 \n",
      "shrimp 318 \n",
      "cuisine 797 \n",
      "bother 623 \n",
      "gloves 941 \n",
      "sour 1735 \n",
      "wasted 1363 \n",
      "onion 1289 \n",
      "fancy 1500 \n",
      "terrific 1346 \n",
      "appetite 1059 \n",
      "caring 823 \n",
      "reasonably 516 \n",
      "anytime 355 \n",
      "homemade 618 \n",
      "hi 1658 \n",
      "convenient 710 \n",
      "lovely 1199 \n",
      "seating 549 \n",
      "blanket 1673 \n",
      "authentic 254 \n",
      "bean 1195 \n",
      "mgm 1572 \n",
      "exceeding 1258 \n",
      "shower 620 \n",
      "moist 717 \n",
      "awkward 1742 \n",
      "curry 685 \n",
      "unbelievable 1046 \n",
      "speedy 965 \n",
      "shoots 1392 \n",
      "servers 271 \n",
      "hottest 1124 \n",
      "ample 981 \n",
      "thrilled 1347 \n",
      "sexy 1121 \n",
      "crisp 1119 \n",
      "descriptions 820 \n",
      "sliced 572 \n",
      "cocktail 853 \n",
      "delight 1452 \n",
      "burger 125 \n",
      "seafood 251 \n",
      "boring 1277 \n",
      "email 1022 \n",
      "tomato 978 \n",
      "bye 524 \n",
      "batter 485 \n",
      "tuna 1553 \n",
      "sandwich 195 \n",
      "pasta 601 \n",
      "trimmed 1245 \n",
      "flavors 1646 \n",
      "upgrading 1213 \n",
      "hooked 800 \n",
      "spice 1491 \n",
      "mayo 499 \n",
      "fry 661 \n",
      "reminds 815 \n",
      "dough 1239 \n",
      "warmer 1680 \n",
      "hated 919 \n",
      "belly 1306 \n",
      "cod 1586 \n",
      "strawberry 793 \n",
      "vain 1358 \n",
      "accountant 1618 \n",
      "clue 1531 \n",
      "inexpensive 489 \n",
      "bamboo 1391 \n",
      "nude 1088 \n",
      "honestly 1603 \n",
      "hurry 907 \n",
      "texture 728 \n",
      "awesome 161 \n",
      "vanilla 803 \n",
      "slices 886 \n",
      "swung 962 \n",
      "chefs 1120 \n",
      "goat 1683 \n",
      "downside 1055 \n",
      "coconut 920 \n",
      "dreamed 1259 \n",
      "peanut 1092 \n",
      "cute 1083 \n",
      "velvet 1042 \n",
      "companions 1560 \n",
      "hut 1273 \n",
      "whatsoever 1145 \n",
      "wow 370 \n",
      "selections 627 \n",
      "patty 1701 \n",
      "neat 1386 \n",
      "grilled 582 \n",
      "fabulous 1479 \n",
      "im 1173 \n",
      "app 1737 \n",
      "pros 1237 \n",
      "sadly 808 \n",
      "polite 1436 \n",
      "crust 488 \n",
      "nan 777 \n",
      "melted 839 \n",
      "privileged 1739 \n",
      "cheek 869 \n",
      "bakery 1033 \n",
      "steak 109 \n",
      "mushrooms 1546 \n",
      "crab 719 \n",
      "genuinely 1108 \n",
      "beautifully 995 \n",
      "tailored 1333 \n",
      "melt 725 \n",
      "mirage 1018 \n",
      "boiled 1481 \n",
      "accommodations 1348 \n",
      "delicious 73 \n",
      "reviewer 1697 \n",
      "seasoned 716 \n",
      "dessert 284 \n",
      "gem 1605 \n",
      "damn 348 \n",
      "toast 887 \n",
      "wasting 537 \n",
      "specials 1015 \n",
      "insults 1611 \n",
      "roast 1515 \n",
      "dinners 1360 \n",
      "rude 273 \n",
      "nyc 1104 \n",
      "mushroom 933 \n",
      "perfection 545 \n",
      "mediocre 250 \n",
      "hamburger 1091 \n",
      "vodka 1324 \n",
      "roasted 674 \n",
      "chow 866 \n",
      "cakes 1329 \n",
      "flair 761 \n",
      "happier 1172 \n",
      "meh 564 \n",
      "ensued 946 \n",
      "ri 1643 \n",
      "annoying 1136 \n",
      "marrow 736 \n",
      "ethic 1061 \n",
      "baba 1713 \n",
      "disgrace 912 \n",
      "peach 1402 \n",
      "peanuts 1533 \n",
      "salsa 474 \n",
      "pumpkin 1221 \n",
      "lobster 1267 \n",
      "rib 914 \n",
      "elk 1474 \n",
      "bland 175 \n",
      "toro 1048 \n",
      "gourmet 1399 \n",
      "haunt 850 \n",
      "vegetarian 409 \n",
      "thinly 1052 \n",
      "caterpillar 929 \n",
      "spices 1458 \n",
      "flavored 988 \n",
      "sunglasses 937 \n",
      "thumbs 600 \n",
      "charcoal 512 \n",
      "appalling 1167 \n",
      "ventilation 1212 \n",
      "humiliated 1183 \n",
      "dime 1441 \n",
      "disgust 1496 \n",
      "grossed 518 \n",
      "edible 388 \n",
      "az 1174 \n",
      "fries 233 \n",
      "pastry 807 \n",
      "disbelief 843 \n",
      "dipping 1179 \n",
      "sewer 1288 \n",
      "nay 1073 \n",
      "cheated 879 \n",
      "flop 1559 \n",
      "bloodiest 1160 \n",
      "screams 916 \n",
      "spicy 186 \n",
      "refreshing 1236 \n",
      "juries 860 \n",
      "enjoyable 1621 \n",
      "waitress 211 \n",
      "grease 508 \n",
      "menus 1659 \n",
      "spinach 1316 \n",
      "buffet 180 \n",
      "cafÃ© 1300 \n",
      "greedy 1439 \n",
      "waiter 212 \n",
      "rave 1165 \n",
      "lettuce 950 \n",
      "imaginative 968 \n",
      "hilarious 1508 \n",
      "cocktails 1449 \n",
      "tasted 182 \n",
      "bathrooms 479 \n",
      "unhealthy 1397 \n",
      "insulted 597 \n",
      "exquisite 1580 \n",
      "yukon 1547 \n",
      "delightful 1725 \n",
      "watered 1181 \n",
      "coupons 1274 \n",
      "allergy 1529 \n",
      "smells 1286 \n",
      "biscuits 466 \n",
      "sushi 132 \n",
      "downright 1275 \n",
      "lukewarm 739 \n",
      "ninja 1093 \n",
      "unwelcome 1253 \n",
      "negligent 1252 \n",
      "oysters 905 \n",
      "meats 961 \n",
      "decor 663 \n",
      "drenched 1648 \n",
      "sucked 689 \n",
      "taco 690 \n",
      "patio 548 \n",
      "vibe 364 \n",
      "spaghetti 1144 \n",
      "gc 1574 \n",
      "creamy 495 \n",
      "eel 821 \n",
      "binge 1340 \n",
      "sever 983 \n",
      "rowdy 1157 \n",
      "pricey 1728 \n",
      "powdered 890 \n",
      "suck 648 \n",
      "hostess 1241 \n",
      "forgetting 1037 \n",
      "bartender 713 \n",
      "disgusting 513 \n",
      "disappoint 711 \n",
      "burgers 402 \n",
      "needless 782 \n",
      "thru 1325 \n",
      "devine 814 \n",
      "handmade 1450 \n",
      "toasted 1008 \n",
      "desserts 393 \n",
      "dine 1691 \n",
      "salads 1127 \n",
      "leftover 1034 \n",
      "combo 1518 \n",
      "tasty 164 \n",
      "teamwork 824 \n",
      "00 778 \n",
      "tenders 780 \n",
      "steaks 446 \n",
      "solidify 1615 \n",
      "mandalay 1315 \n",
      "sorely 902 \n",
      "sauces 822 \n",
      "impeccable 546 \n",
      "blah 1538 \n",
      "despicable 1062 \n",
      "ala 1519 \n",
      "iced 1056 \n",
      "seasoning 973 \n",
      "palate 1334 \n",
      "dripping 1720 \n",
      "smelled 830 \n",
      "saffron 972 \n",
      "flirting 1123 \n",
      "stale 611 \n",
      "uploaded 1705 \n",
      "attentive 255 \n",
      "spotty 1687 \n",
      "calligraphy 1204 \n",
      "inflate 1094 \n",
      "raspberry 852 \n",
      "venturing 1305 \n",
      "informative 1415 \n",
      "puree 707 \n",
      "moods 1383 \n",
      "cranberry 1588 \n",
      "mellow 932 \n",
      "fireball 760 \n",
      "pears 1732 \n",
      "almonds 1733 \n",
      "screwed 1619 \n",
      "fluffy 1224 \n",
      "soups 1736 \n",
      "lastly 1250 \n",
      "veal 1427 \n",
      "cashier 733 \n",
      "delights 1314 \n",
      "awkwardly 1066 \n",
      "soggy 515 \n",
      "pancakes 637 \n",
      "sucker 1403 \n",
      "eggplant 660 \n",
      "biscuit 1244 \n",
      "rinse 1086 \n",
      "capers 1107 \n",
      "disgraceful 1617 \n",
      "avocado 1317 \n",
      "cotta 1462 \n",
      "nutshell 1284 \n",
      "fo 857 \n",
      "smeared 1408 \n",
      "pancake 786 \n",
      "unbelievably 872 \n",
      "etc 1626 \n",
      "brownish 1554 \n",
      "pecan 1222 \n",
      "entrees 679 \n",
      "char 629 \n",
      "compliments 1696 \n",
      "mein 867 \n",
      "mains 1715 \n",
      "greasy 1396 \n",
      "overpriced 458 \n",
      "hiro 1451 \n",
      "brunch 1361 \n",
      "letdown 1139 \n",
      "crepe 842 \n",
      "vinaigrette 959 \n",
      "mozzarella 1251 \n",
      "dusted 889 \n",
      "customize 1193 \n",
      "firehouse 1416 \n",
      "atrocious 1327 \n",
      "ambience 544 \n",
      "ache 910 \n",
      "sucks 454 \n",
      "appetizer 1453 \n",
      "crusty 755 \n",
      "pizzas 1485 \n",
      "pneumatic 1078 \n",
      "ironman 856 \n",
      "truffle 1054 \n",
      "bagels 1105 \n",
      "nicest 531 \n",
      "stinks 798 \n",
      "revisiting 1000 \n",
      "muffin 1010 \n",
      "fs 892 \n",
      "crispy 1297 \n",
      "tasteless 244 \n",
      "tacos 310 \n",
      "halibut 985 \n",
      "flavorful 380 \n",
      "yum 497 \n",
      "mesquite 945 \n",
      "meatballs 979 \n",
      "uninspired 1716 \n",
      "redeeming 1429 \n",
      "jalapeno 784 \n",
      "driest 1551 \n",
      "petrified 1354 \n",
      "waitresses 790 \n",
      "appetizers 751 \n",
      "sugary 1331 \n",
      "chipotle 960 \n",
      "brisket 957 \n",
      "pita 670 \n",
      "khao 1279 \n",
      "personable 1484 \n",
      "raving 1330 \n",
      "kabuki 1688 \n",
      "replenished 771 \n",
      "summarize 1072 \n",
      "dont 468 \n",
      "ambiance 339 \n",
      "temp 938 \n",
      "subpar 1677 \n",
      "cashew 1388 \n",
      "deliciously 1478 \n",
      "fillet 1592 \n",
      "bartenders 762 \n",
      "outrageously 1122 \n",
      "tartar 1486 \n",
      "refill 529 \n",
      "pucks 1495 \n",
      "styrofoam 840 \n",
      "excalibur 1153 \n",
      "bbq 1430 \n",
      "cant 715 \n",
      "pho 242 \n",
      "chewy 436 \n",
      "fiancÃ© 935 \n",
      "tapas 749 \n",
      "crawfish 743 \n",
      "dispenser 1080 \n",
      "mortified 1209 \n",
      "cheeseburger 521 \n",
      "fella 1669 \n",
      "rudely 1641 \n",
      "filet 1475 \n",
      "soi 1280 \n",
      "tummy 900 \n",
      "condiment 1079 \n",
      "poop 1413 \n",
      "tots 616 \n",
      "mouthful 1620 \n",
      "scallop 656 \n",
      "yummy 312 \n",
      "violinists 1201 \n",
      "underwhelming 624 \n",
      "jenni 1310 \n",
      "legit 552 \n",
      "guacamole 1503 \n",
      "hummus 671 \n",
      "unsatisfying 781 \n",
      "insanely 1730 \n",
      "milkshake 1246 \n",
      "fondue 1699 \n",
      "combos 1031 \n",
      "carbs 1342 \n",
      "margaritas 1161 \n",
      "disrespected 1442 \n",
      "camelback 1140 \n",
      "unwrapped 1417 \n",
      "buffets 1414 \n",
      "madhouse 1494 \n",
      "slaw 1647 \n",
      "penne 1323 \n",
      "yama 1235 \n",
      "bitches 1374 \n",
      "gyro 948 \n",
      "undercooked 569 \n",
      "sashimi 373 \n",
      "omg 501 \n",
      "gratuity 1567 \n",
      "croutons 1084 \n",
      "nachos 482 \n",
      "calamari 1644 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nobu 1694 \n",
      "godfathers 1652 \n",
      "panna 1461 \n",
      "trippy 1387 \n",
      "thats 756 \n",
      "lox 1106 \n",
      "ians 954 \n",
      "overcooked 511 \n",
      "crema 1299 \n",
      "omelets 1434 \n",
      "skimp 1684 \n",
      "yellowtail 1540 \n",
      "gringos 1170 \n",
      "tartare 1049 \n",
      "moz 1674 \n",
      "sangria 1393 \n",
      "refried 753 \n",
      "gristle 1745 \n",
      "boba 1308 \n",
      "gyros 589 \n",
      "bruschetta 813 \n",
      "5lb 1743 \n",
      "tiramisu 747 \n",
      "doughy 966 \n",
      "carpaccio 1541 \n",
      "callings 1187 \n",
      "baklava 1711 \n",
      "tater 1068 \n",
      "yucky 773 \n",
      "flavorless 967 \n",
      "anyways 1656 \n",
      "spicier 1351 \n",
      "huevos 1670 \n",
      "cannoli 748 \n",
      "brushfire 1419 \n",
      "outshining 984 \n",
      "rancheros 1671 \n",
      "soooo 785 \n",
      "bouchon 1493 \n",
      "lordy 1278 \n",
      "choux 806 \n",
      "fav 1444 \n",
      "mmmm 1589 \n",
      "bachi 634 \n",
      "ohhh 1044 \n",
      "wagyu 1053 \n",
      "ribeye 944 \n",
      "rge 1591 \n",
      "wontons 1583 \n",
      "sooooo 1182 \n",
      "blandest 795 \n",
      "crÃªpe 1563 \n",
      "heimer 1303 \n",
      "unexperienced 1024 \n",
      "relleno 1593 \n",
      "40min 1130 \n",
      "steiners 1601 \n",
      "kiddos 1082 \n",
      "eew 951 \n",
      "definately 693 \n",
      "untoasted 1011 \n",
      "delicioso 1016 \n",
      "sause 1180 \n",
      "sals 1345 \n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "\n",
    "filePath = 'GloVe_PreTrained/glove.6B.50d.txt'\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(filePath,\n",
    "                                           tokenizer.word_index, \n",
    "                                           embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.18000013e-01  2.49679998e-01 -4.12420005e-01  1.21699996e-01\n",
      "   3.45270008e-01 -4.44569997e-02 -4.96879995e-01 -1.78619996e-01\n",
      "  -6.60229998e-04 -6.56599998e-01  2.78430015e-01 -1.47670001e-01\n",
      "  -5.56770027e-01  1.46579996e-01 -9.50950012e-03  1.16579998e-02\n",
      "   1.02040000e-01 -1.27920002e-01 -8.44299972e-01 -1.21809997e-01\n",
      "  -1.68009996e-02 -3.32789987e-01 -1.55200005e-01 -2.31309995e-01\n",
      "  -1.91809997e-01 -1.88230002e+00 -7.67459989e-01  9.90509987e-02\n",
      "  -4.21249986e-01 -1.95260003e-01  4.00710011e+00 -1.85939997e-01\n",
      "  -5.22870004e-01 -3.16810012e-01  5.92130003e-04  7.44489999e-03\n",
      "   1.77780002e-01 -1.58969998e-01  1.20409997e-02 -5.42230010e-02\n",
      "  -2.98709989e-01 -1.57490000e-01 -3.47579986e-01 -4.56370004e-02\n",
      "  -4.42510009e-01  1.87849998e-01  2.78489990e-03 -1.84110001e-01\n",
      "  -1.15139998e-01 -7.85809994e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 100, 50)           87350     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 96, 128)           32128     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 120,779\n",
      "Trainable params: 120,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "model3.add(layers.Embedding(vocab_size, \n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=True)) # Make it False\n",
    "#model3.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model3.add(layers.GlobalMaxPool1D())\n",
    "\n",
    "model3.add(layers.Dense(10, activation='relu'))\n",
    "model3.add(layers.Dense(1, activation='sigmoid'))\n",
    "model3.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 750 samples, validate on 250 samples\n",
      "Epoch 1/20\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.6697 - acc: 0.5960 - val_loss: 0.6099 - val_acc: 0.6680\n",
      "Epoch 2/20\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.4372 - acc: 0.8240 - val_loss: 0.5314 - val_acc: 0.7080\n",
      "Epoch 3/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3170 - acc: 0.8960 - val_loss: 0.5553 - val_acc: 0.6960\n",
      "Epoch 4/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1790 - acc: 0.9640 - val_loss: 0.5824 - val_acc: 0.7280\n",
      "Epoch 5/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0976 - acc: 0.9893 - val_loss: 0.4821 - val_acc: 0.7720\n",
      "Epoch 6/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0435 - acc: 0.9973 - val_loss: 0.5218 - val_acc: 0.7600\n",
      "Epoch 7/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0187 - acc: 0.9987 - val_loss: 0.5617 - val_acc: 0.7600\n",
      "Epoch 8/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0103 - acc: 0.9987 - val_loss: 0.5535 - val_acc: 0.7800\n",
      "Epoch 9/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0065 - acc: 0.9987 - val_loss: 0.5982 - val_acc: 0.7760\n",
      "Epoch 10/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.6042 - val_acc: 0.7760\n",
      "Epoch 11/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.6436 - val_acc: 0.7760\n",
      "Epoch 12/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.6305 - val_acc: 0.7840\n",
      "Epoch 13/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.6595 - val_acc: 0.7800\n",
      "Epoch 14/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.6631 - val_acc: 0.7800\n",
      "Epoch 15/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.6777 - val_acc: 0.7840\n",
      "Epoch 16/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 8.8061e-04 - acc: 1.0000 - val_loss: 0.6881 - val_acc: 0.7800\n",
      "Epoch 17/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 7.4532e-04 - acc: 1.0000 - val_loss: 0.7038 - val_acc: 0.7880\n",
      "Epoch 18/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 6.3471e-04 - acc: 1.0000 - val_loss: 0.7225 - val_acc: 0.7800\n",
      "Epoch 19/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 5.5039e-04 - acc: 1.0000 - val_loss: 0.7275 - val_acc: 0.7840\n",
      "Epoch 20/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 4.7836e-04 - acc: 1.0000 - val_loss: 0.7294 - val_acc: 0.7840\n"
     ]
    }
   ],
   "source": [
    "history3 = model3.fit(X_train, y_train,\n",
    "                    epochs=20,verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model3.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7840\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model3.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
