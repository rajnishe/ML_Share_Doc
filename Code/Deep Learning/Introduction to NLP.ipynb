{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: Wall Street Journal>\n"
     ]
    }
   ],
   "source": [
    "print(text7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw='Program is specifically designed for IT professionals. Also Freshers who are looking for a job in premier IT companies. This Program is a good program'\n",
    "\n",
    "#Tokenization\n",
    "sent_tokens = nltk.sent_tokenize(raw)\n",
    "word_tokens = nltk.word_tokenize(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Program is specifically designed for IT professionals.', 'Also Freshers who are looking for a job in premier IT companies.', 'This Program is a good program']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Program', 'is', 'specifically', 'designed', 'for', 'IT', 'professionals', '.', 'Also', 'Freshers', 'who', 'are', 'looking', 'for', 'a', 'job', 'in', 'premier', 'IT', 'companies', '.', 'This', 'Program', 'is', 'a', 'good', 'program']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_word_tokens = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_word_tokens.append(w)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(word_tokens) - len(filtered_word_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program\n",
      "is\n",
      "specif\n",
      "design\n",
      "for\n",
      "IT\n",
      "profession\n",
      ".\n",
      "Also\n",
      "Fresher\n",
      "who\n",
      "are\n",
      "look\n",
      "for\n",
      "a\n",
      "job\n",
      "in\n",
      "premier\n",
      "IT\n",
      "compani\n",
      ".\n",
      "Thi\n",
      "Program\n",
      "is\n",
      "a\n",
      "good\n",
      "program\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(raw)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Program', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('specifically', 'RB'),\n",
       " ('designed', 'VBN'),\n",
       " ('for', 'IN'),\n",
       " ('IT', 'PRP'),\n",
       " ('professionals', 'VBZ'),\n",
       " ('.', '.'),\n",
       " ('Also', 'RB'),\n",
       " ('Freshers', 'NNP'),\n",
       " ('who', 'WP'),\n",
       " ('are', 'VBP'),\n",
       " ('looking', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('job', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('premier', 'JJR'),\n",
       " ('IT', 'PRP'),\n",
       " ('companies', 'VBZ'),\n",
       " ('.', '.'),\n",
       " ('This', 'DT'),\n",
       " ('Program', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('good', 'JJ'),\n",
       " ('program', 'NN')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parts of Speech Tagging\n",
    "\n",
    "\n",
    "nltk.pos_tag(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "statement = 'Gaurav lives in Ghaziabad and teaches at ITBodhi'\n",
    "tokens = nltk.word_tokenize(statement)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "namedEnt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization: \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))\n",
    "\n",
    "# lemmatize takes a part of speech parameter, \"pos.\" If not supplied, the default is \"noun.\" \n",
    "#This means that an attempt will be made to find the closest noun, which can create trouble for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer().fit(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 17)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 14)\t2\n",
      "  (2, 16)\t1\n"
     ]
    }
   ],
   "source": [
    "# transform the documents in the training data to a document-term matrix\n",
    "sent_tokens_vectorized = vect.transform(sent_tokens)\n",
    "\n",
    "print(sent_tokens_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['also', 'are', 'companies', 'designed', 'for', 'freshers', 'good', 'in', 'is', 'it', 'job', 'looking', 'premier', 'professionals', 'program', 'specifically', 'this', 'who']\n"
     ]
    }
   ],
   "source": [
    "print (vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 18)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 15)\t0.433816094216\n",
      "  (0, 14)\t0.329928321457\n",
      "  (0, 13)\t0.433816094216\n",
      "  (0, 9)\t0.329928321457\n",
      "  (0, 8)\t0.329928321457\n",
      "  (0, 4)\t0.329928321457\n",
      "  (0, 3)\t0.433816094216\n",
      "  (1, 17)\t0.313777337045\n",
      "  (1, 12)\t0.313777337045\n",
      "  (1, 11)\t0.313777337045\n",
      "  (1, 10)\t0.313777337045\n",
      "  (1, 9)\t0.238635752576\n",
      "  (1, 7)\t0.313777337045\n",
      "  (1, 5)\t0.313777337045\n",
      "  (1, 4)\t0.238635752576\n",
      "  (1, 2)\t0.313777337045\n",
      "  (1, 1)\t0.313777337045\n",
      "  (1, 0)\t0.313777337045\n",
      "  (2, 16)\t0.452123308263\n",
      "  (2, 14)\t0.687702859235\n",
      "  (2, 8)\t0.343851429617\n",
      "  (2, 6)\t0.452123308263\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
    "vect = TfidfVectorizer().fit(sent_tokens)\n",
    "\n",
    "sent_tokens_vectorized = vect.transform(sent_tokens)\n",
    "\n",
    "print(sent_tokens_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm = sent_tokens_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.43381609  0.32992832  0.          0.\n",
      "   0.          0.32992832  0.32992832  0.          0.          0.\n",
      "   0.43381609  0.32992832  0.43381609  0.          0.        ]\n",
      " [ 0.31377734  0.31377734  0.31377734  0.          0.23863575  0.31377734\n",
      "   0.          0.31377734  0.          0.23863575  0.31377734  0.31377734\n",
      "   0.31377734  0.          0.          0.          0.          0.31377734]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.45212331  0.          0.34385143  0.          0.          0.          0.\n",
      "   0.          0.68770286  0.          0.45212331  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x18 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 22 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
